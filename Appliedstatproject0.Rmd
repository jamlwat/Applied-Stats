---
title: "Applied Stats Module 0 Project- 2024 GAC Softball Hitting Statistics"
author: "Jami Watson"
date: "2025-02-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
df<-read.csv("https://raw.githubusercontent.com/nurfnick/Data_Sets_For_Stats/refs/heads/master/CuratedDataSets/24GAChittingstatsUpdate.csv", fileEncoding = "UTF-8")

head(df)
```

```{r}
head(df)
```
I found this data at https://static.greatamericanconference.com/custompages/SOFTBALL%20Stats/2024/lgplyrs.htm. For this project we will be focusing on the quantitative variables RBIs, games played (GP), and games started (GS), and the categorical variables team and starters (GP/GS).

# Module 0
```{r}
summary(df)
```
Summary statistics of quantitative variable RBI's
```{r}
summary(df$RBI)
```
Standard Deviation of RBI's
```{r}
sd(df$RBI, na.rm = TRUE)
```
```{r}
hist(df$RBI, 
     main = "Histogram of RBI", 
     xlab = "RBI", 
     ylab = "Frequency", 
     col = "blue", 
     border = "black")
```

```{r}
boxplot(df$RBI, 
        main = "Box Plot of RBI", 
        ylab = "Values", 
        col = "lightblue", 
        border = "black", 
        horizontal = FALSE)
```
```{r}
qqnorm(df$RBI, main = "QQ Plot of RBI")
qqline(df$RBI, col = "red", lwd = 2)  # Adds a reference line
```

From the above graphical displays, I found that there were multiple outliers within the data and the distribution was heavily skewed. This is most likely due to the data including all players in the GAC, including starters, non-starters, and non-hitters.

Frequency and relative frequency tables for categorical variable, Teams. These values should closely align with roster count of each team in the GAC in the 2024 season.
```{r}
freq_table <- table(df$Team)
print(freq_table)
```
```{r}
rel_freq_table <- prop.table(freq_table)
print(rel_freq_table)
```
Two way table comparing games played and games started to determine number of starters on each team. A player is considered a starter if they start in 75% of games played. To be clear, this does not guarantee all starters are hitters, but it narrows down our data.
```{r}
df$Starters<-df$GS/df$GP>0.75
```
```{r}
table(df$Team,df$Starters)
```
Null Hypothesis- True mean is equivalent to 15 RBIs

Alternative Hypothesis- True mean is not equivalent to 15 RBIs
```{r}
t.test(df$RBI,mu=15)
```
We received a low p-value so we reject the null hypothesis. The true mean of RBIs among all players is not 15. 

Null Hypothesis- True mean among starters is equivalent to 15 RBIs

Alternative Hypothesis- True mean among starters is not equivalent to 15 RBIs
```{r}
t.test(df[df$Starters,"RBI"],mu=15)
```
We received a low p-value so we reject the null hypothesis. The true mean of RBIs among starters is not 15. However, I must say my estimate was very close to the true mean.

Null Hypothesis- True difference in means of RBIs of starters and non-starters is 0

Alternative Hypothesis- True difference in means of RBIs of starters and non-starters is not 0
```{r}
t.test(df[df$Starters,"RBI"],df[!df$Starters,"RBI"])
```
We received a low p-value so we reject the null hypothesis. The true difference in means of RBIs of starters and non-starters is not 0.


In conclusion, there is a significant difference in the mean number of RBIs of starters and non-starters, which accounts for why our initial data distribution of RBIs among players was so skewed. The mean number of RBIs among starters was not 15, however, I was very close with my estimate as the true mean was 15.929688. I suppose next I will remove non-hitters from my data set and see what results I get then.

# Module 1

```{r}


plot(df$Hits, df$AVG, 
     main = "Scatter Plot", 
     xlab = "Hits", 
     ylab = "AVG", 
     col = "blue", 
     pch = 19)

model <- lm(AVG ~ Hits, data = df)


abline(model, col = "red", lwd = 2)


```


```{r}
intercept <- coef(model)[1]
slope <- coef(model)[2]

eq <- paste0("y = ", round(intercept, 5), " + ", round(slope, 5), "x")
print(eq)
```

```{r}
cor(df$Hits, df$AVG)
cor.test(df$Hits, df$AVG)
```
While this does show a positive relationship between hits and batting average, it is not a very strong relationship. Lets look at and compare the correlations between and hits and batting average of starters and non starters.
```{r}
ggplot(data = df, aes(x = Hits,y = AVG, colour = Starters))+
  geom_jitter()+
  geom_smooth(method = 'lm')

```
We can clearly see that among starters, the relationship between hits and batting average is strong. However, among non starters the relationship is very weak, as we would expect.
```{r}
predict(model,df[df$Player == "Watson",],interval="confidence")

model$residuals[43]

```
Players actual average was 0.297 which is within our interval.
```{r}
predict(model,data.frame(Hits = c(50)))
```
Based on our regression, a player with 50 hits would have a 0.352 batting average.
```{r}
plot(model)
```

```{r}
summary(model)
```
```{r}
lm(AVG~.-Player-SB.ATT,df)
```

```{r}
aggregate(HBP ~ Team, data = df, FUN = sum)
```
Total HBP of each team in the GAC

# Module 2
We will now perform a bootstrap mean on RBIs and compare this to our mean we computed early on in Module 0. Original Hypothesis test:
Null Hypothesis- True mean is equivalent to 15 RBIs
Alternative Hypothesis- True mean is not equivalent to 15 RBIs
```{r}
library(boot)
boot_mean <- function(df, indices) {
  return(mean(df[indices]))
}
set.seed(123)
bootstrap_results <- boot(df$RBI, boot_mean, R = 1001)
print(bootstrap_results)
```
Our original mean was 9.232 and the difference between our original mean and our bootstrap mean is 0.02373.This means our bootstrap mean is 9.255 RBIs.
```{r}
hist(bootstrap_results$t, breaks = 30, main = "Bootstrap Means RBI", xlab = "Mean Value", col = "lightblue", border = "black")
```

```{r}
boot.ci(bootstrap_results, type = c("perc", "bca"))
```
The 95% confidence interval for the mean is approximately between 7.989 and 10.663 RBIs.Since our null hypothesis of 15 RBI's does not lie within our interval, we can reject the null hypothesis.

For my new hypothesis test:
Null hypothesis- True median is equivalent to 7 BB's (walks).
Alternative hypothesis- True median is not equivalent to 7 BB's (walks).
```{r}
library(boot)
boot_median <- function(df, indices) {
  return(median(df[indices]))
}
set.seed(123)
bootstrap_results <- boot(df$BB, boot_median, R = 1001)
print(bootstrap_results)
```
My estimate was obviously pretty far off, I think if we could run the bootstrap mean on starters only it would have been more on target.
```{r}
hist(bootstrap_results$t, breaks = 30, main = "Bootstrap Median BB's", xlab = "Median Value", col = "lightblue", border = "black")
```
Unlike our bootstrap mean distribution, this distribution is not normal at all, which is to be expected.
```{r}
boot.ci(bootstrap_results, type = c("perc", "bca"))
```
My mean estimate is not in the confidence interval, thus we can reject my null hypothesis. The true median is not equivalent to 8 walks.

Onto Cross validation
```{r}
library(caret)
trainingsample <- createDataPartition(df$Hits, p=0.667,list=FALSE)
```
Randomly selected 2/3 of the data to be my training data.
```{r}
traindata <- df[trainingsample, ]
testdata <- df[-trainingsample, ]
```
Assigned this training data to traindata, and assigned the rest, the remaining 1/3, to be testdata.
```{r}
model <- lm(AVG ~ Hits, data = traindata)
summary(model)
```
Created a linear model of our train data. Our r squared isn't great, which makes sense because our original r squared wasn't great.
```{r}
plot(traindata$Hits,traindata$AVG,col="red")
points(testdata$Hits,testdata$AVG,col="blue")
abline(model)
```
Plotted our traindata (red) and testdata (blue) in a scatter plot along with our linear model.
```{r}
pre <- predict(model, testdata)
```
```{r}
R2(pre, testdata$AVG)
```
Examined our predication on our test data.
```{r}
train.control <- trainControl(method="cv", number=10)
model <- train(AVG~Hits, 
               data=df,
               method='lm',
               trControl=train.control)
print(model)
```
Performed a 10-fold cross validation. Tbh idk what exactly this is telling me....

# Module 3
```{r}
max(df$BB)
```
So our categories will be 0-6 walks, 7-13 walks, 14-20 walks, 21-27 walks, and 28-34 walks. So lets test the theory that p1=p2=p3=p4=p5.Note: I am testing this, however I know I will have to reject my hypothesis, I just didn't have many categorical variables to work with.

Null Hypothesis: Players are equally likely to have a total walk count in each category.
Alternative Hypothesis: Players are not equally likely to have a total walk count in each category.

First we will find our observed counts
```{r}
bins <- c(0, 7, 14, 21, 28, 35)

walk_categories <- cut(df$BB, breaks = bins, right = FALSE)

observed_counts <- table(walk_categories)

observed_counts
```
Our expected counts will be 51.8 for all bins.
```{r}
observed <- c(159, 62, 20, 14, 4)


expected <- rep(51.8, 5)


chisq.test(x = observed, p = rep(0.20, 5))


chisq.test(x = observed, p = expected / sum(expected))
```
We got a very very small p-value, thus we can reject our null hypothesis.
```{r}
counts_matrix <- rbind(Observed = observed, Expected = expected)
bin_labels <- c("0-6", "7-13", "14-20", "21-17", "28-34")

barplot(counts_matrix, 
        beside = TRUE,
        names.arg = bin_labels,
        col = c("skyblue", "salmon"), 
        main = "Observed vs Expected Counts",
        xlab = "Walk Range",
        ylab = "Walk Count",
        legend.text = TRUE)
```
I will do this over starters and non-starters and whether this is independent of number of hits. 
Null hypothesis: There is no association between whether a player is a starter and whether they have greater than 30 hits.
Alternative hypothesis: There is association between whether a player is a starter and whether they have greater than 30 hits.
Lets start with getting our observed counts
```{r}
observed_counts <- table(df$Starters, df$Hits>25)

observed_counts
```
```{r}
chisq.test(observed_counts)
```
We got a very low p-value, thus we can reject our null hypothesis. Proving there is an association between whether a player is a starter and whether they have greater than 30 hits.
```{r}
chisq.test(observed_counts)$expected
```


```{r}
mosaicplot(observed_counts,
           main = "Mosaic Starter vs Hits>30",
           color = TRUE,
           shade = TRUE)
```
I need help reading this tbh....

# Module 4

One way- Averages of Teams
Null Hypothesis: True mean of averages for each team are equal.
Alternative: True mean of averages for each team are not equal.
```{r}
results<- aov(AVG~Team, data=df)
summary(results)
```
We fail to reject the null hypothesis, thus the true mean of averages for each team are equal.
```{r}
plot(results)
```
Residual variance isn't great but also isn't bad.
```{r}
ggplot(df, aes(y=AVG, x=Team))+ 
  geom_boxplot()
```
two way- Averages of starters and non starters on a team
Null hypothesis: Mean of averages of Starters is equal to mean of averages of non-starters on each team.
Alternative hypothesis: Mean of averages of starters is not equal to mean of averages of non-starters on each team.
```{r}
result<- aov(AVG~Team*Starters, data=df)
summary(result)
```
Reject null on two way table, being a starter makes a difference and teams and starters makes a difference.
```{r}
ggplot(df, aes(y=AVG, x=Team, fill=Starters))+ 
  geom_boxplot()
```
You can really see the difference being a starter makes here. 

kruskal-wallis (anova but with median) and spearman rank correlation (module 1)

# Module 5       

Alrighty, for module 5 I will be first be performing the kruskal-wallis test on the same thing I did for my one-way anova test, but I will be using the non-parametric statistic, median, instead of mean.

Null Hypothesis: Median of averages for each team are equal.
Alternative Hypothesis: Median of averages for each team are not equal.

First, lets look at the boxplot.
```{r}
boxplot(df$AVG ~ df$Team)
```
Harding and SNU have two of the highest team batting averages, and they were ranked highly in the conference, so it makes sense that they are up there.

Now lets look at the median batting averages for each team.
```{r}
by(df$AVG,df$Team, median)
```
Looks about right.

Now we run our test
```{r}
kruskal.test(AVG ~ Team , data = df)
```
We got a pretty high p-value, thus we fail to reject our null hypothesis. So, the median of averages for each team are equal. We got the same result when looking at the means. 

```{r}
barplot(table(df$AVG),
        main = "AVG Distribution",
        xlab = "AVG", ylab = "Count")
```
Not normal

I think the kruskal wallis test was the better way to go because the data did not have normal distribution, nor could I guarantee equal variance across the groups. And in this case since all the data is pretty close together and there is a lot of it, the medians probably don't lie very far from the means anyways.

Okayyyy, now onto the Spearman rank correlation test. I will perform the test on the same thing that I did in module 1, that is, do hits relate to avg. SPOILER ALERT, THEY DO!

But anyways here we go. 

```{r}
cor.test(df$Hits, df$AVG, method = "spearman")
```
We reject the null hypothesis- very low p-value

```{r}
plot(df$Hits, df$AVG)
abline(lm(AVG ~ Hits, data = df),col = "Blue")
```
Obviously we get a very positive, strong correlation. Hits do relate to AVG.


```{r}
barplot(table(df$Hits),
        main = "Hit distribution",
        xlab = "Hits", ylab = "Count")
```
Obviously not normal

```{r}
barplot(table(df$AVG),
        main = "Batting Average Distribution",
        xlab = "AVG", ylab = "Count")
```
Not normal


In this particular case, I think Spearman is the way to go because there are outliers in this data. Number of hits is obviously related to batting average because number of hits is used to calculate batting average. The outliers in this data are negligible, they are one off instances and don't truly exemplify an accurate batting average. Why should a pitchers .000 batting average hold weight, or someone who pinch hit once and got a hit so they are batting 1.000. They don't reflect a real batting average, which is why in this case spearman is better because the outliers don't effect our result as much. The outliers here are negligible, which is why when I used perason I ran the test on starters so I could prove the relationship existed. 

Also, neither distributions of are normal because of the outliers in the data, so spearman is a better test.